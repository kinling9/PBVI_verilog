### POMDP
1. 首先从知乎回答[优雅搞懂PBVI](https://zhuanlan.zhihu.com/p/168779238)中给出的决策tiger问题。
2. 对于该问题，首先给出该问题已知的内容和目标

- 第一个要明确的内容是，对于决策者，当前状态是完全未知的，当然，如果知道了当前状态，那么决策者永远可以做出reward最大化的决策，因此也不需要一个决策过程
- 首先是核心的转移矩阵，即在某个状态$s_i$下，行为$a_i$会有多少概率导致下一个状态为$s_{i+1}$<br>
其中$P_{1,1} + P_{1,2} + P_{1,3} = 1，这一公式保证每个状态$s_i$均有合适的转移状态$s_{i+1}$，同时，由于行为对于状态转移存在影响，不同的行为将会有不同的转移矩阵
$$For ~Action~a$$
$$
\left\{
\begin{matrix}
P_{1,1} & P_{1,2} & P_{1,3} \\
P_{2,1} & P_{2,2} & P_{2,3} \\
P_{3,1} & P_{3,2} & P_{3,3} 
\end{matrix}
\right\}
$$
> 例如对于tiger问题，假如观测行为对于老虎的状态没有影响，那么该行为对应的状态转移矩阵为：
$$
\left\{
\begin{matrix}
1 & 0  \\
0 & 1  
\end{matrix}
\right\}
$$
- 随后是行为$a_i$带来的信息，对于tiger问题，观测行为听到的声音会为决策者带来一定的信息，因此通过该信息观测者可以对于接下来的行为$a_{i+1}$进行决策以期获得最大的reward，同时，由于干扰的存在，观察结果并不是确定的。因此观察矩阵可以认为是不同的状态下会观测到不同的结果的概率
> 同样以文中提到的tiger问题为例，其观察矩阵可以表示为：
$$
\left\{
\begin{matrix}
0.85 & 0.15  \\
0.15 & 0.85  
\end{matrix}
\right\}
$$
- 最后是行为对应的reward矩阵，每一个行为均对应于该状态某行为对应的reward
> 对于tiger问题，一个合适的reward可以设定为:
$$
\left\{
\begin{matrix}
10 & -1 & -100  \\
-100 & -1 & 10 
\end{matrix}
\right\}
$$
> 这一reward即当打开正确的门，会得到100分的reward，观测只需要花费1的reward，而开错门则会消耗100的reward

3. 随后是该问题的输出
- 由于该问题随着迭代深度的增加，其最终准确表达式的复杂程度是指数级上涨的，因此我们选择使用$k$个采样点上的值用于最终的决策过程，这$k$个点上的决策决定了对于一个确定的置信概率，应该做出怎样的决策


4. 最后，是如何从已知的内容中得到最终的输出结果。
- 首先，我们要对这个问题进行不断加深的模拟，对于每一种决策，我们可以根据当前对于当前状态的置信程度算出对应的期望，因此对于每一个点，我们获知了每一种决策在不同点上对应的期望值。
- 在第一次的过程结束后，我们会根据第一次的行为获取一定的观测结果，然后对于不同的观测结果，我们会进行置信程度的转移，那么我们新的置信结果将会由观测矩阵和转移矩阵决定，我们首先计算我们置信概率随状态转义矩阵的变化，随后将各状态的结果分别乘上新的观测概率<br>（比如两次在右侧听的行为均观测到老虎在左侧，那么我们可以认为老虎在左侧的可能性将会非常小）
- 上述置信向量的更新过程不仅可以应用于模拟过程中，同样可以应用于最终输出的过程中。
- 第一次模拟的过程比较直接，后续的递归过程将需要根据前述结果进行更新，这里仅对PVBI中的算法进行解释，首先我们不考虑置信向量，直接计算本次观测和下一次行为导致的reward更新。
  - 对于所有的采样点，我们都会枚举其对应的$\{a,o,s\}$对应的全部reward
  - 随后，为了能够使得该代价能够指导决策过程，我们需要将后续模拟的值函数向前级决策进行转移，从而可以通过最终的reward值指导最初的决策过程，转移函数如下：
    $$
    Reward_{a,o,s(i)} = \sum_{s(i-1) \in S} Reward_{s(i-1)} * P(s_i,s_{i-1}) * P(o,s_{i-1})
    $$
  - 由于前述计算内容的限制以及点数的限制，我们只能从采样点处获得${a,o}$对应的信息
  - 考虑到任意行为后的观测结果是无法预知的，因此合适的期望应当是所有观测结果的和，同时，考虑到PVBI的决策是选择具有最大值的点，这里模拟该决策过程，从而得到不同行为对应的结果
- 当我们获得到不同决策点上不同行为对应的reward值后，最终的决策就很简单了，我们只要选择每个置信点上的最大化价值行为即可。

 - 由于$\{a,s\}$已知，我们可以直接获得本次行为$a$的代价$reward_a$
5. 对于实际的马尔可夫决策过程的模拟
- 首先，对于一个决策过程，给定一个初始的置信向量，即决策者初始认为的状态概率分布情况：
> 如果在tiger问题中，决策者认为老虎在两扇门后的概率均为50%，则初始的置信状态为
$$
\left\{
\begin{matrix}
0.5 & 0.5
\end{matrix}
\right\}
$$
- 随后，是最终模拟的输出，即一个合理的决策过程即我们应当在什么时候选择怎样的行为。
